{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVRwR8OvAHWh"
   },
   "source": [
    "# Image text recognition (from text-only images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Dataset\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn import CTCLoss\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing a few inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "for filename in glob.glob('/mjsynth/90kDICT32px/1/1/*.jpg'):\n",
    "    im=Image.open(filename)\n",
    "    image_list.append(im)\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,(i+1))\n",
    "    plt.imshow(image_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCcMCyHZkRrQ"
   },
   "source": [
    "# Training data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_vhsRk6aT2-"
   },
   "outputs": [],
   "source": [
    "class create_dataset(Dataset):\n",
    "    ## finds and loads the data and its annotations, given the mjsynth directory \n",
    "    all_chars = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "    charToLabel = {char: i + 1 for i, char in enumerate(all_chars)}\n",
    "    labelToChar = {label: char for char, label in charToLabel.items()}\n",
    "\n",
    "    def __init__(self, dir_=None, mode=None, img_path=None, img_ht=32, img_wdt=100):\n",
    "        if dir_ and mode and not img_path:\n",
    "            img_path, texts = self.__load_files__(dir_, mode)\n",
    "        elif not dir_ and not mode and img_path:\n",
    "            texts = None\n",
    "\n",
    "        self.img_path = img_path\n",
    "        self.texts = texts\n",
    "        self.img_ht = img_ht\n",
    "        self.img_wdt = img_wdt\n",
    "\n",
    "    def __load_files__(self, dir_, mode):\n",
    "        mapping = {}\n",
    "        with open(os.path.join(dir_, 'lexicon.txt'), 'r') as fr:\n",
    "            for i, line in enumerate(fr.readlines()):\n",
    "                mapping[i] = line.strip()\n",
    "\n",
    "        annotation_file = None\n",
    "        if mode == 'train':\n",
    "            annotation_file = 'annotation_train.txt'\n",
    "        elif mode == 'dev':\n",
    "            annotation_file = 'annotation_val.txt'\n",
    "        elif mode == 'test':\n",
    "            annotation_file = 'annotation_test.txt'\n",
    "\n",
    "        img_path = []\n",
    "        texts = []\n",
    "        with open(os.path.join(dir_, annotation_file), 'r') as fr:\n",
    "            for line in fr.readlines():\n",
    "                path, index_str = line.strip().split(' ')\n",
    "                path = os.path.join(dir_, path)\n",
    "                index = int(index_str)\n",
    "                text = mapping[index]\n",
    "                img_path.append(path)\n",
    "                texts.append(text)\n",
    "        return img_path, texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.img_path[index]\n",
    "\n",
    "        try:\n",
    "            image = Image.open(path).convert('L')  # grey-scale\n",
    "        except IOError:\n",
    "            return self[index + 1]\n",
    "\n",
    "        ## reshaping and scaling the input images\n",
    "        \n",
    "        image = image.resize((self.img_wdt, self.img_ht), resample=Image.BILINEAR)\n",
    "        image = np.array(image)\n",
    "        image = image.reshape((1, self.img_ht, self.img_wdt))\n",
    "        image = (image / 127.5) - 1.0\n",
    "\n",
    "        image = torch.FloatTensor(image)\n",
    "        if self.texts:     # if in training/validation modes  \n",
    "            text = self.texts[index]\n",
    "            target = [self.charToLabel[c] for c in text]\n",
    "            target_length = [len(target)]\n",
    "\n",
    "            target = torch.LongTensor(target)\n",
    "            target_length = torch.LongTensor(target_length)\n",
    "            return image, target, target_length\n",
    "        else:              # if in testing mode\n",
    "            return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate function for dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fun(batch): ## used to create the labels for the images\n",
    "    images, targets, target_len = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    targets = torch.cat(targets, 0)\n",
    "    target_len = torch.cat(target_len, 0)\n",
    "    return images, targets, target_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ivAHQWsiRXC"
   },
   "source": [
    "# model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHFu-qyjiQ9v"
   },
   "outputs": [],
   "source": [
    "class my_model(nn.Module):\n",
    "    def __init__(self,img_channels,img_ht,img_w,num_class,map_to_seq_hidden = 64,rnn_hidden = 256):\n",
    "        super(my_model, self).__init__()\n",
    "        self.cnn_stack = nn.Sequential(\n",
    "                OrderedDict([\n",
    "                  ('conv1', nn.Conv2d(img_channels,64,3,1,1)),\n",
    "                  # ('bn1',nn.BatchNorm2d(64)),\n",
    "                  ('relu1', nn.ReLU(inplace=True)),\n",
    "                  ('pool1',nn.MaxPool2d(kernel_size = 2,stride = 2)),\n",
    "\n",
    "                  ('conv2', nn.Conv2d(64,128,3,1,1)),\n",
    "                  # ('bn2',nn.BatchNorm2d(128)),\n",
    "                  ('relu2', nn.ReLU(inplace=True)),\n",
    "                  ('pool2',nn.MaxPool2d(kernel_size = 2,stride = 2)),\n",
    "\n",
    "                  ('conv3', nn.Conv2d(128,256,3,1,1)),\n",
    "                  # ('bn3',nn.BatchNorm2d(128)),\n",
    "                  ('relu3', nn.ReLU(inplace=True)),\n",
    "\n",
    "                  ('conv4', nn.Conv2d(256,256,3,1,1)),\n",
    "                  # ('bn4',nn.BatchNorm2d(256)),\n",
    "                  ('relu4', nn.ReLU(inplace=True)),\n",
    "                  ('pool4',nn.MaxPool2d(kernel_size = (2,1))),\n",
    "\n",
    "                  ('conv5', nn.Conv2d(256,512,3,1,1)),\n",
    "                  ('relu5', nn.ReLU(inplace=True)),\n",
    "                  ('bn5',nn.BatchNorm2d(512)),\n",
    "\n",
    "                  ('conv6', nn.Conv2d(512,512,3,1,1)),\n",
    "                  ('relu6', nn.ReLU(inplace=True)),\n",
    "                  ('bn6',nn.BatchNorm2d(512)),\n",
    "                  ('pool6',nn.MaxPool2d(kernel_size = (2,1))),\n",
    "\n",
    "                  ('conv7', nn.Conv2d(512,512,2,1,0)),\n",
    "                  ('relu7', nn.ReLU(inplace=True)),\n",
    "                ])\n",
    "            )\n",
    "        out_ht = img_ht // 16 - 1\n",
    "        out_w = img_w // 4 -1\n",
    "        self.map_to_seq = nn.Linear(512*out_ht, map_to_seq_hidden)\n",
    "        self.lstm1 = nn.LSTM(map_to_seq_hidden, rnn_hidden, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(2 * rnn_hidden, rnn_hidden, bidirectional=True)\n",
    "        self.out_ = nn.Linear(2 * rnn_hidden, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.cnn_stack(x)\n",
    "        batch, channel, height, width = conv.size()\n",
    "        conv = conv.view(batch, channel * height, width)\n",
    "        conv = conv.permute(2, 0, 1)  # (width, batch, feature)\n",
    "        seq = self.map_to_seq(conv)\n",
    "        lstm_, _ = self.lstm1(seq)\n",
    "        lstm_, _ = self.lstm2(lstm_)\n",
    "        out = self.out_(lstm_)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHnbigyziS7v"
   },
   "source": [
    "# Character decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0Ec6mNriU1L"
   },
   "outputs": [],
   "source": [
    "def decode(log_probs,labelToChar = None,blank = 0):\n",
    "    emission_log_probs = np.transpose(log_probs.cpu().numpy(), (1, 0, 2)) # (batch, length, class)\n",
    "    decoded_list = []\n",
    "    for emission_log_prob in emission_log_probs:\n",
    "        labels = np.argmax(emission_log_prob, axis=-1)\n",
    "        new_labels = []\n",
    "        # merging same labels\n",
    "        previous = None\n",
    "        for l in labels:\n",
    "            if l != previous:\n",
    "                new_labels.append(l)\n",
    "                previous = l\n",
    "        decoded = []\n",
    "        ## removing blanks\n",
    "        for i in new_labels:\n",
    "            if i!=blank:\n",
    "                decoded.append(i)\n",
    "        ## for prediction, convert the number predictions to characters\n",
    "        if labelToChar:\n",
    "            decoded = [labelToChar[l] for l in decoded]\n",
    "        decoded_list.append(decoded)\n",
    "    return decoded_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZPv3-Bpi0mP"
   },
   "source": [
    "# evalute the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBT0YQ25i2fG"
   },
   "outputs": [],
   "source": [
    "def evaluate(model_, dataloader, criterion):\n",
    "    model_.eval()\n",
    "\n",
    "    count = 0\n",
    "    eval_loss = 0\n",
    "    num_correct_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            device = 'cuda' if next(model_.parameters()).is_cuda else 'cpu'\n",
    "\n",
    "            images, targets, target_len = [d.to(device) for d in data]\n",
    "\n",
    "            logits = model_(images)\n",
    "            log_probs = torch.nn.functional.log_softmax(logits, dim=2)\n",
    "\n",
    "            batch_sz = images.size(0)\n",
    "            input_lengths = torch.LongTensor([logits.size(0)] * batch_sz)\n",
    "\n",
    "            loss = criterion(log_probs, targets, input_lengths, target_len)\n",
    "\n",
    "            preds = decode(log_probs)\n",
    "            reals = targets.cpu().numpy().tolist()\n",
    "            target_len = target_len.cpu().numpy().tolist()\n",
    "\n",
    "            count += batch_sz\n",
    "            eval_loss += loss.item()\n",
    "            target_length_counter = 0\n",
    "            for pred, target_length in zip(preds, target_len):\n",
    "                real = reals[target_length_counter:target_length_counter + target_length]\n",
    "                target_length_counter += target_length\n",
    "                if pred == real:\n",
    "                    num_correct_preds += 1\n",
    "\n",
    "    eval_ = {\n",
    "        'loss': eval_loss / count,\n",
    "        'acc': num_correct_preds / count,\n",
    "    }\n",
    "    return eval_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvyLNEDsdacz"
   },
   "source": [
    "# Train function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model_, data, optimizer, criterion, device):\n",
    "    model_.train()\n",
    "    images, targets, target_len = [x.to(device) for x in data]\n",
    "\n",
    "    logits = model_(images)\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=2)\n",
    "\n",
    "    batch_sz = images.size(0)\n",
    "    input_lengths = torch.LongTensor([logits.size(0)] * batch_sz)\n",
    "    target_len = torch.flatten(target_len)\n",
    "\n",
    "    loss = criterion(log_probs, targets, input_lengths, target_len)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_batch_sz = 32\n",
    "eval_batch_sz = 512\n",
    "lr = 0.0005\n",
    "show_train_loss = 20000\n",
    "show_valid_loss = 50000\n",
    "save_model = 50000\n",
    "cpu_workers = 4\n",
    "check_pt =  '/scratch/pm3140/checkpoints/check_pt1350000.pt'\n",
    "# check_pt = None\n",
    "\n",
    "img_wdt = 100\n",
    "img_ht = 32\n",
    "data_path = '/mjsynth/90kDICT32px/'\n",
    "checkpts_path = '/scratch/pm3140/checkpoints/'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_dataset(dir_=data_path, mode='train',\n",
    "                                img_ht=img_ht, img_wdt=img_wdt)\n",
    "val_data = create_dataset(dir_=data_path, mode='dev',\n",
    "                                img_ht=img_ht, img_wdt=img_wdt)\n",
    "\n",
    "tr_loader = DataLoader(dataset=train_data,batch_size=train_batch_sz,\n",
    "    shuffle=True,num_workers=cpu_workers,collate_fn=collate_fun)\n",
    "val_loader = DataLoader(dataset=val_data,batch_size=eval_batch_sz,\n",
    "    shuffle=True,num_workers=cpu_workers,collate_fn=collate_fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(create_dataset.labelToChar) + 1\n",
    "model_ = my_model(1, img_ht, img_wdt, num_class,\n",
    "            map_to_seq_hidden=64,rnn_hidden=256)\n",
    "if check_pt:\n",
    "    model_.load_state_dict(torch.load(check_pt, map_location=device))\n",
    "model_.to(device)\n",
    "\n",
    "optimizer = optim.RMSprop(model_.parameters(), lr=lr)\n",
    "criterion = CTCLoss(reduction='sum')\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_hist = []\n",
    "val_loss_hist = []\n",
    "val_acc_hist = []\n",
    "i = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print('epoch: ',epoch)\n",
    "    tot_train_loss = 0.0\n",
    "    tot_train_count = 0.0\n",
    "    for train_data in tr_loader:\n",
    "        loss = train_batch(model_, train_data, optimizer, criterion, device)\n",
    "        train_size = train_data[0].size(0)\n",
    "\n",
    "        tot_train_loss += loss\n",
    "        tot_train_count += train_size\n",
    "        \n",
    "        if i % show_train_loss == 0:\n",
    "            train_loss_hist.append(loss / train_size)\n",
    "            print('train_batch_loss[%d]: %4f\\n'%(i,loss / train_size))\n",
    "\n",
    "        if i % show_valid_loss == 0:\n",
    "            print('evaluating on the validation set ...')\n",
    "            eval_ = evaluate(model_, val_loader, criterion)\n",
    "            val_loss_hist.append(eval_['loss'])\n",
    "            val_acc_hist.append(eval_['acc'])\n",
    "            print('valid: loss=%4f, acc=%4f'%(eval_['loss'],eval_['acc']))\n",
    "\n",
    "        if i % save_model == 0:\n",
    "            loss = eval_['loss']\n",
    "            print('saving model ...')\n",
    "            torch.save(model_.state_dict(), \n",
    "                       os.path.join(checkpts_path,'check_pt'+str(i)+'.pt'))\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    print('total train loss: ', tot_train_loss / tot_train_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Accuracy Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(train_loss_hist[0:80],label='Train')\n",
    "ax.plot(val_loss_hist[0:80],label='Validation')\n",
    "plt.title('Train and Validation loss')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "ax.legend()\n",
    "# plt.legend([a,b],['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10), dpi=80)\n",
    "fig, ax = plt.subplots()\n",
    "# ax.plot(val_loss[0:80],label='Loss')\n",
    "ax.plot(val_acc_hist,label='accuracy')\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('accuracy')\n",
    "ax.legend()\n",
    "# plt.legend([a,b],['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_acc_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNNObgC_d5Yi"
   },
   "source": [
    "## Prediction function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDYTwV27O6P4"
   },
   "outputs": [],
   "source": [
    "def predict(model_, dataloader, labelToChar):\n",
    "    model_.eval()\n",
    "    final_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            device = 'cuda' if next(model_.parameters()).is_cuda else 'cpu'\n",
    "            images = data.to(device)\n",
    "            logits = model_(images)\n",
    "            log_probs = torch.nn.functional.log_softmax(logits, dim=2)\n",
    "            preds = decode(log_probs,labelToChar)\n",
    "            final_pred+=preds\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Prediction paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [os.path.join(pth, f) for pth, dir_s, files in os.walk('/scratch/pm3140/test_images') for f in files]\n",
    "# images = ['/scratch/pm3140/test_images/100_Classmates_13991.jpg']\n",
    "img_ht = 32\n",
    "img_wdt = 100\n",
    "check_pt='/scratch/pm3140/checkpoints/check_pt1350000.pt'\n",
    "# check_pt = None\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device}')\n",
    "\n",
    "\n",
    "pred_data = create_dataset(img_path = images,img_ht=img_ht, img_wdt=img_wdt)\n",
    "\n",
    "pred_loader = DataLoader(dataset=pred_data,shuffle=False,num_workers=4)\n",
    "\n",
    "num_class = len(create_dataset.labelToChar) + 1\n",
    "model_ = my_model(1, img_ht, img_wdt, num_class,map_to_seq_hidden=64,rnn_hidden=256)\n",
    "model_.load_state_dict(torch.load(check_pt, map_location=device))\n",
    "model_.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(model_, pred_loader, create_dataset.labelToChar)\n",
    "for pred in preds:\n",
    "    final_pred = ''.join(pred)\n",
    "    print(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project-Model.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "7e6d6622369332e3a84f3e8ffa94ddde599246cffa3e9c8c4bd94ca653694390"
  },
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
